#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gestionnaire de m√©tadonn√©es et index pour DataLake NBA
Cr√©e et maintient les index globaux et m√©tadonn√©es consolid√©es
"""

import os
import json
import glob
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd

class MetadataManager:
    """Gestionnaire des m√©tadonn√©es et index du DataLake"""
    
    def __init__(self, data_dir: str = 'data'):
        self.data_dir = data_dir
        self.metadata_dir = f'{data_dir}/metadata'
        self.raw_dir = f'{data_dir}/raw'
        
        # Cr√©ation des dossiers n√©cessaires
        os.makedirs(self.metadata_dir, exist_ok=True)
    
    def scan_data_files(self) -> Dict[str, List[str]]:
        """Scanne tous les fichiers de donn√©es et les classe par type"""
        data_files = {
            'api_nba': [],
            'kaggle': [],
            'processed': [],
            'metadata': []
        }
        
        # Scan des fichiers API NBA
        api_files = glob.glob(f'{self.raw_dir}/api_nba/*.csv')
        data_files['api_nba'] = [os.path.basename(f) for f in api_files]
        
        # Scan des fichiers Kaggle
        kaggle_files = glob.glob(f'{self.raw_dir}/kaggle/*.csv')
        data_files['kaggle'] = [os.path.basename(f) for f in kaggle_files]
        
        # Scan des fichiers trait√©s
        processed_files = glob.glob(f'{self.data_dir}/processed/**/*.csv', recursive=True)
        data_files['processed'] = [os.path.relpath(f, self.data_dir) for f in processed_files]
        
        # Scan des m√©tadonn√©es
        metadata_files = glob.glob(f'{self.metadata_dir}/session_*.json')
        data_files['metadata'] = [os.path.basename(f) for f in metadata_files]
        
        return data_files
    
    def create_data_index(self) -> Dict[str, Any]:
        """Cr√©e l'index global des donn√©es"""
        data_files = self.scan_data_files()
        
        # Calcul des statistiques
        total_files = sum(len(files) for files in data_files.values())
        total_size = 0
        
        # Calcul de la taille totale
        for category, files in data_files.items():
            for file in files:
                if category == 'processed':
                    file_path = f'{self.data_dir}/{file}'
                elif category == 'metadata':
                    file_path = f'{self.metadata_dir}/{file}'
                else:
                    file_path = f'{self.raw_dir}/{category}/{file}'
                
                if os.path.exists(file_path):
                    total_size += os.path.getsize(file_path)
        
        # Cr√©ation de l'index
        data_index = {
            'created_at': datetime.now().isoformat(),
            'total_files': total_files,
            'total_size_bytes': total_size,
            'total_size_mb': round(total_size / (1024 * 1024), 2),
            'categories': {
                'api_nba': {
                    'count': len(data_files['api_nba']),
                    'files': data_files['api_nba']
                },
                'kaggle': {
                    'count': len(data_files['kaggle']),
                    'files': data_files['kaggle']
                },
                'processed': {
                    'count': len(data_files['processed']),
                    'files': data_files['processed']
                },
                'metadata': {
                    'count': len(data_files['metadata']),
                    'files': data_files['metadata']
                }
            },
            'last_updated': datetime.now().isoformat()
        }
        
        return data_index
    
    def consolidate_session_metadata(self) -> Dict[str, Any]:
        """Consolide les m√©tadonn√©es de toutes les sessions"""
        metadata_files = glob.glob(f'{self.metadata_dir}/session_*.json')
        
        if not metadata_files:
            return {}
        
        all_sessions = []
        total_players = 0
        total_teams = 0
        total_seasons = 0
        total_errors = 0
        
        for metadata_file in metadata_files:
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    session_data = json.load(f)
                    all_sessions.append(session_data)
                    
                    # Accumulation des statistiques
                    if 'collected_data' in session_data:
                        data = session_data['collected_data']
                        
                        if 'players_static' in data:
                            total_players = max(total_players, data['players_static'].get('count', 0))
                        
                        if 'teams_static' in data:
                            total_teams = max(total_teams, data['teams_static'].get('count', 0))
                        
                        if 'player_career_stats' in data:
                            total_seasons += data['player_career_stats'].get('total_seasons', 0)
                        
                        if 'team_season_stats' in data:
                            total_seasons += data['team_season_stats'].get('total_seasons', 0)
                    
                    if 'errors' in session_data:
                        total_errors += len(session_data['errors'])
                        
            except Exception as e:
                print(f"Erreur lecture {metadata_file}: {e}")
                continue
        
        # Tri par date de cr√©ation
        all_sessions.sort(key=lambda x: x.get('start_time', ''), reverse=True)
        
        # M√©tadonn√©es consolid√©es
        consolidated_metadata = {
            'created_at': datetime.now().isoformat(),
            'total_sessions': len(all_sessions),
            'latest_session': all_sessions[0] if all_sessions else None,
            'statistics': {
                'total_players_collected': total_players,
                'total_teams_collected': total_teams,
                'total_seasons_collected': total_seasons,
                'total_errors': total_errors
            },
            'sessions': all_sessions,
            'last_updated': datetime.now().isoformat()
        }
        
        return consolidated_metadata
    
    def update_all_metadata(self):
        """Mise √† jour compl√®te de toutes les m√©tadonn√©es et index"""
        try:
            print("Mise √† jour des m√©tadonn√©es et index...")
            
            # Mise √† jour des m√©tadonn√©es globales
            metadata_result = self.update_global_metadata()
            
            # Mise √† jour de l'index des donn√©es
            index_result = self.update_data_index()
            
            print(f"\nMise √† jour termin√©e avec succ√®s!")
            print(f"  - M√©tadonn√©es: {'‚úÖ' if metadata_result['success'] else '‚ùå'}")
            print(f"  - Index: {'‚úÖ' if index_result['success'] else '‚ùå'}")
            
            return {
                'metadata': metadata_result,
                'index': index_result
            }
            
        except Exception as e:
            error_msg = f"Erreur lors de la mise √† jour compl√®te: {e}"
            print(f"‚ùå {error_msg}")
            return {
                'success': False,
                'error': error_msg
            }

if __name__ == "__main__":
    # Test du gestionnaire
    manager = MetadataManager()
    result = manager.update_all_metadata()
    print(f"\nüéØ Mise √† jour termin√©e avec succ√®s!")
