# NBA DataLake - Architecture et Analytics IA

## Vue d'ensemble du projet

Ce projet implÃ©mente une solution d'architecture DataLake complÃ¨te pour l'analyse des donnÃ©es NBA, combinant des sources de donnÃ©es multiples et des capacitÃ©s d'intelligence artificielle.

## Architecture technique

### Couche 1 : Ingestion
- **Dataset Kaggle NBA** : DonnÃ©es historiques structurÃ©es (30 Ã©quipes, 4800+ joueurs, 65k+ matchs)
- **API NBA officielle** : DonnÃ©es temps rÃ©el et mises Ã  jour
- **Pipeline ETL** : Extraction, transformation et chargement automatisÃ©

### Couche 2 : Persistance
- **Base de donnÃ©es relationnelle** : Stockage des donnÃ©es structurÃ©es
- **SystÃ¨me de mÃ©tadonnÃ©es** : Gestion des informations de traÃ§abilitÃ©
- **Indexation automatique** : Optimisation des requÃªtes analytiques

### Couche 3 : Insight et Analytics
- **Dashboard interactif** : Visualisations et analyses en temps rÃ©el
- **Rapports automatisÃ©s** : GÃ©nÃ©ration d'insights significatifs
- **Intelligence artificielle** : PrÃ©dictions et dÃ©couverte de patterns

## MÃ©thodologie

### Approche de dÃ©veloppement
1. **Analyse des besoins** : Identification des sources de donnÃ©es et cas d'usage
2. **Conception architecturale** : DÃ©finition des couches et composants
3. **ImplÃ©mentation itÃ©rative** : DÃ©veloppement par phases avec tests continus
4. **Validation des rÃ©sultats** : Comparaison avec Ã©tudes existantes

### Sources de donnÃ©es
- **Kaggle Dataset** : DonnÃ©es historiques NBA depuis 1946-47
- **API NBA officielle** : Statistiques temps rÃ©el et mises Ã  jour
- **DonnÃ©es complÃ©mentaires** : Articles, analyses, mÃ©tadonnÃ©es

### Limitations et gestion des erreurs

#### API NBA officielle
**Limitations connues :**
- **Rate limiting** : L'API limite le nombre de requÃªtes par minute
- **Timeouts frÃ©quents** : Erreurs `HTTPSConnectionPool: Read timed out` communes
- **Serveurs surchargÃ©s** : ParticuliÃ¨rement pendant les heures de pointe
- **Protection anti-bot** : DÃ©tection des collectes massives

**Gestion des erreurs :**
- **Timeout configurÃ©** : 30 secondes par requÃªte
- **DÃ©lai entre requÃªtes** : 0.3 secondes pour Ã©viter la surcharge
- **Collecte par lots** : Limitation Ã  300 joueurs actifs par session
- **Retry automatique** : Gestion des erreurs temporaires

**Recommandations :**
- Collecter pendant les heures creuses (nuit US)
- Utiliser des pauses entre les sessions de collecte
- Surveiller les logs pour identifier les patterns d'erreur
- ConsidÃ©rer la collecte incrÃ©mentale pour les mises Ã  jour

#### Dataset Kaggle
- **Taille** : ~4.4 GB de donnÃ©es historiques
- **Mise Ã  jour** : Manuel via Kaggle CLI
- **FiabilitÃ©** : Excellente, pas de limitations de rate

### Technologies utilisÃ©es
- **Backend** : Python, FastAPI, SQLAlchemy
- **Base de donnÃ©es** : PostgreSQL
- **Frontend** : Dash, Plotly
- **IA/ML** : Scikit-learn, TensorFlow (selon besoins)

## Installation et utilisation

### PrÃ©requis
- Python 3.8+
- PostgreSQL
- Git

### Installation
```bash
# Cloner le repository
git clone <repository-url>
cd tp_group

# CrÃ©er l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### Configuration
1. CrÃ©er un fichier `.env` avec les variables d'environnement
2. Configurer la base de donnÃ©es PostgreSQL
3. Lancer les services d'ingestion et de dashboard

## Structure du projet

```
tp_group/
â”œâ”€â”€ data/                 # DonnÃ©es organisÃ©es selon l'architecture DataLake
â”‚   â”œâ”€â”€ ingestion/        # Couche d'ingestion des donnÃ©es
â”‚   â”‚   â”œâ”€â”€ api_nba/      # DonnÃ©es API NBA officielle (0.27 MB)
â”‚   â”‚   â”œâ”€â”€ kaggle/       # Dataset Kaggle basketball (4.4 GB)
â”‚   â”‚   â”œâ”€â”€ scraping/     # Web scraping (prÃ©parÃ©)
â”‚   â”‚   â””â”€â”€ external/     # Autres sources externes
â”‚   â”œâ”€â”€ persistence/      # Couche de stockage et gestion
â”‚   â”‚   â”œâ”€â”€ raw/          # DonnÃ©es brutes non transformÃ©es
â”‚   â”‚   â”œâ”€â”€ processed/    # DonnÃ©es nettoyÃ©es et transformÃ©es
â”‚   â”‚   â”œâ”€â”€ validated/    # DonnÃ©es validÃ©es et qualifiÃ©es
â”‚   â”‚   â””â”€â”€ archived/     # DonnÃ©es archivÃ©es par date
â”‚   â”œâ”€â”€ insight/          # Couche d'analyse et visualisation
â”‚   â”‚   â”œâ”€â”€ analytics/    # Analyses et mÃ©triques
â”‚   â”‚   â”œâ”€â”€ ml_models/    # ModÃ¨les de machine learning
â”‚   â”‚   â”œâ”€â”€ reports/      # Rapports et visualisations
â”‚   â”‚   â””â”€â”€ dashboards/   # Dashboards interactifs
â”‚   â”œâ”€â”€ metadata_globale.json    # MÃ©tadonnÃ©es du projet
â”‚   â”œâ”€â”€ index_donnees.json       # Inventaire des donnÃ©es
â”‚   â””â”€â”€ ORGANISATION_RESUMEE.md  # RÃ©sumÃ© de l'organisation
â”œâ”€â”€ docs/                 # Documentation et Ã©tudes
â”œâ”€â”€ src/                  # Code source
â”‚   â”œâ”€â”€ ingestion/        # Modules de collecte de donnÃ©es
â”‚   â”œâ”€â”€ persistence/      # Modules de stockage et gestion
â”‚   â”œâ”€â”€ insight/          # Modules d'analyse et ML
â”‚   â””â”€â”€ utils/            # Utilitaires communs
â”œâ”€â”€ tests/                # Tests automatisÃ©s
â”œâ”€â”€ requirements.txt      # DÃ©pendances Python
â””â”€â”€ README.md            # Ce fichier
```

## Organisation des donnÃ©es

### Architecture DataLake implÃ©mentÃ©e
Le projet suit une architecture DataLake en 3 couches principales :

#### ğŸ“¥ Couche INGESTION
- **API NBA** : DonnÃ©es officielles temps rÃ©el (joueurs, Ã©quipes, statistiques)
- **Dataset Kaggle** : DonnÃ©es historiques complÃ¨tes (1946-2023)
- **Sources externes** : PrÃ©parÃ©es pour futures intÃ©grations

#### ğŸ’¾ Couche PERSISTENCE  
- **Raw** : DonnÃ©es brutes non transformÃ©es
- **Processed** : DonnÃ©es nettoyÃ©es et transformÃ©es
- **Validated** : DonnÃ©es qualifiÃ©es et validÃ©es
- **Archived** : Historique et archivage par date

#### ğŸ” Couche INSIGHT
- **Analytics** : MÃ©triques et analyses
- **ML Models** : ModÃ¨les prÃ©dictifs et IA
- **Reports** : Rapports automatisÃ©s
- **Dashboards** : Visualisations interactives

### DonnÃ©es disponibles
- **API NBA** : 5 fichiers CSV (0.27 MB) - donnÃ©es actuelles
- **Kaggle** : 17 fichiers CSV + SQLite (4.4 GB) - donnÃ©es historiques
- **TraÃ§abilitÃ©** : MÃ©tadonnÃ©es complÃ¨tes et index des donnÃ©es

## Cas d'usage

### Analyse des performances
- Comparaison des statistiques joueurs
- Ã‰volution des performances saisonniÃ¨res
- Analyse des matchs et rÃ©sultats

### PrÃ©dictions et insights
- PrÃ©diction des rÃ©sultats de matchs
- Identification des patterns de jeu
- Analyse des tendances historiques

### Reporting automatisÃ©
- Rapports de performance hebdomadaires
- Analyses comparatives d'Ã©quipes
- Suivi des records et statistiques

## Ã‰tat actuel du projet

### âœ… RÃ©alisÃ©
- **Architecture DataLake** : Structure complÃ¨te implÃ©mentÃ©e
- **Organisation des donnÃ©es** : Nettoyage et organisation terminÃ©s
- **Sources de donnÃ©es** : API NBA et Kaggle intÃ©grÃ©es
- **Documentation** : MÃ©tadonnÃ©es et index crÃ©Ã©s
- **Menu principal** : Interface de gestion dÃ©veloppÃ©e

### ğŸš§ En cours
- **IntÃ©gration des donnÃ©es** : Fusion API NBA + Kaggle
- **Couche persistence** : Base de donnÃ©es et stockage
- **Dashboard** : Interface de visualisation

### ğŸ“‹ Ã€ venir
- **Analytics avancÃ©es** : MÃ©triques et KPIs
- **Machine Learning** : ModÃ¨les prÃ©dictifs
- **Rapports automatisÃ©s** : GÃ©nÃ©ration d'insights

## Configuration et monitoring

### Fichier de configuration
Le projet utilise `config.py` pour centraliser la configuration :
```python
NBA_API_CONFIG = {
    "timeout": 30,           # Timeout des requÃªtes API
    "delay": 0.3,            # DÃ©lai entre requÃªtes
    "max_players": 300,      # Limite de joueurs par session
    "start_year": 2000,      # AnnÃ©e de dÃ©but des donnÃ©es
    "seasons": list(range(2000, 2026))  # Saisons Ã  collecter
}
```

### Monitoring des collectes
- **Logs dÃ©taillÃ©s** : `data/ingestion.log`
- **MÃ©tadonnÃ©es des sessions** : `data/metadata/session_*.json`
- **Index des donnÃ©es** : `data/index_donnees.json`
- **Gestion des erreurs** : Timeouts et retry automatiques

### Bonnes pratiques
- **Collecte par lots** : Ã‰viter la surcharge de l'API
- **Pauses entre sessions** : Respecter les limitations de rate
- **Surveillance des logs** : Identifier les patterns d'erreur
- **Backup des donnÃ©es** : Sauvegarde avant nouvelles collectes

## Contribution

Ce projet suit les bonnes pratiques de dÃ©veloppement :
- Code documentÃ© en franÃ§ais
- Tests automatisÃ©s
- Architecture modulaire et extensible
- Documentation technique complÃ¨te
- Organisation DataLake conforme aux standards

## Licence

Projet acadÃ©mique - Tous droits rÃ©servÃ©s
